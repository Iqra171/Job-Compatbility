{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yb8OO6VZ46Nx",
        "outputId": "f32b5fc9-42e1-4669-ea77-2724916be8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All packages installed successfully!\n",
            "Loading Sentence Transformer model...\n",
            "‚úÖ Model loaded successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1679794164.py:708: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=custom_css, title=\"Enhanced Resume Analyzer\") as demo:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "üöÄ Launching Enhanced Resume Analyzer...\n",
            "==================================================\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d1f136ea92ce454705.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d1f136ea92ce454705.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2202, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1924, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1879, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: A  function (clear_all) didn't return enough output values (needed: 11, returned: 10).\n",
            "    Output components:\n",
            "        [file, textbox, markdown, markdown, markdown, markdown, markdown, markdown, markdown, markdown, textbox]\n",
            "    Output values returned:\n",
            "        [None, \"\n",
            "\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n"
          ]
        }
      ],
      "source": [
        "# NLP-Based Resume-Job Description Compatibility Checker with Enhanced Features\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q gradio sentence-transformers PyPDF2 nltk scikit-learn python-docx\n",
        "\n",
        "import gradio as gr\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "from io import BytesIO\n",
        "import io\n",
        "from docx import Document\n",
        "from collections import Counter\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "# ============================================\n",
        "# 1. LOAD EMBEDDING MODEL\n",
        "# ============================================\n",
        "print(\"Loading Sentence Transformer model...\")\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# ============================================\n",
        "# 2. TEXT EXTRACTION FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"Extract text from uploaded PDF file\"\"\"\n",
        "    try:\n",
        "        if isinstance(pdf_file, str):\n",
        "            with open(pdf_file, 'rb') as f:\n",
        "                pdf_reader = PyPDF2.PdfReader(f)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "        else:\n",
        "            if isinstance(pdf_file, bytes):\n",
        "                pdf_file = io.BytesIO(pdf_file)\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting PDF: {str(e)}\"\n",
        "\n",
        "def extract_text_from_docx(docx_file):\n",
        "    \"\"\"Extract text from uploaded Word (.docx) file\"\"\"\n",
        "    try:\n",
        "        if isinstance(docx_file, str):\n",
        "            doc = Document(docx_file)\n",
        "        else:\n",
        "            if isinstance(docx_file, bytes):\n",
        "                docx_file = io.BytesIO(docx_file)\n",
        "            doc = Document(docx_file)\n",
        "\n",
        "        text = \"\"\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + \"\\n\"\n",
        "\n",
        "        for table in doc.tables:\n",
        "            for row in table.rows:\n",
        "                for cell in row.cells:\n",
        "                    text += cell.text + \" \"\n",
        "            text += \"\\n\"\n",
        "\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting Word document: {str(e)}\"\n",
        "\n",
        "def extract_text_from_file(file):\n",
        "    \"\"\"Universal function to extract text from PDF or Word files\"\"\"\n",
        "    if file is None:\n",
        "        return \"No file uploaded\"\n",
        "\n",
        "    if isinstance(file, str):\n",
        "        filename = file\n",
        "    else:\n",
        "        filename = file.name if hasattr(file, 'name') else \"\"\n",
        "\n",
        "    file_ext = filename.lower().split('.')[-1]\n",
        "\n",
        "    if file_ext == 'pdf':\n",
        "        return extract_text_from_pdf(file)\n",
        "    elif file_ext in ['docx', 'doc']:\n",
        "        return extract_text_from_docx(file)\n",
        "    else:\n",
        "        return f\"Unsupported file format: .{file_ext}. Please upload PDF or Word (.docx) files.\"\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s.,;:()\\-]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# ============================================\n",
        "# 3. NEW FEATURE: BIAS & VAGUE LANGUAGE DETECTOR\n",
        "# ============================================\n",
        "\n",
        "def identify_section_at_position(resume_text, position):\n",
        "    \"\"\"Identify which section of the resume a position falls into\"\"\"\n",
        "    text_lower = resume_text.lower()\n",
        "\n",
        "    # Define section patterns with their identifiers\n",
        "    section_headers = [\n",
        "        ('experience', r'(work experience|professional experience|experience|employment history|work history)'),\n",
        "        ('education', r'(education|academic background|qualifications|degree)'),\n",
        "        ('skills', r'(technical skills|skills|core competencies|expertise|technologies)'),\n",
        "        ('projects', r'(projects|key projects|notable projects|portfolio)'),\n",
        "        ('summary', r'(summary|professional summary|profile|objective|about)'),\n",
        "        ('certifications', r'(certifications|certificates|licenses)'),\n",
        "        ('achievements', r'(achievements|accomplishments|awards|honors)')\n",
        "    ]\n",
        "\n",
        "    # Find all section headers and their positions\n",
        "    sections = []\n",
        "    for section_name, pattern in section_headers:\n",
        "        for match in re.finditer(pattern, text_lower):\n",
        "            sections.append({\n",
        "                'name': section_name,\n",
        "                'start': match.start(),\n",
        "                'end': match.end()\n",
        "            })\n",
        "\n",
        "    # Sort sections by position\n",
        "    sections.sort(key=lambda x: x['start'])\n",
        "\n",
        "    # Determine which section the position falls into\n",
        "    for i, section in enumerate(sections):\n",
        "        section_start = section['end']\n",
        "        section_end = sections[i + 1]['start'] if i + 1 < len(sections) else len(resume_text)\n",
        "\n",
        "        if section_start <= position < section_end:\n",
        "            return section['name'].title()\n",
        "\n",
        "    # If not in any identified section\n",
        "    return \"Other\"\n",
        "\n",
        "def detect_vague_language(resume_text):\n",
        "    \"\"\"Detect weak, vague, or biased phrases in resume with section identification\"\"\"\n",
        "    vague_patterns = {\n",
        "        'weak_verbs': {\n",
        "            'patterns': [\n",
        "                r'\\b(responsible for|duties included|worked on|helped with|assisted in|involved in|participated in)\\b',\n",
        "            ],\n",
        "            'message': 'Weak action verb - replace with strong action verbs',\n",
        "            'examples': 'Use: Led, Developed, Achieved, Implemented, Designed instead'\n",
        "        },\n",
        "        'vague_quantifiers': {\n",
        "            'patterns': [\n",
        "                r'\\b(many|several|various|numerous|multiple|some)\\b',\n",
        "            ],\n",
        "            'message': 'Vague quantifier - add specific numbers',\n",
        "            'examples': 'Replace \"many\" with \"15+\" or \"dozens of\"'\n",
        "        },\n",
        "        'unclear_impact': {\n",
        "            'patterns': [\n",
        "                r'\\b(improved|enhanced|optimized|increased|reduced)\\b(?!\\s+by\\s+\\d)',\n",
        "            ],\n",
        "            'message': 'Missing quantifiable impact - add percentages or metrics',\n",
        "            'examples': 'Add: \"by 35%\", \"by $50K\", \"from 2hrs to 30min\"'\n",
        "        },\n",
        "        'passive_voice': {\n",
        "            'patterns': [\n",
        "                r'\\b(was|were|been)\\s+\\w+ed\\b',\n",
        "            ],\n",
        "            'message': 'Passive voice detected - use active voice',\n",
        "            'examples': 'Change \"was developed by me\" to \"Developed\"'\n",
        "        },\n",
        "        'filler_words': {\n",
        "            'patterns': [\n",
        "                r'\\b(very|really|quite|somewhat|fairly|rather)\\b',\n",
        "            ],\n",
        "            'message': 'Filler word - remove for stronger impact',\n",
        "            'examples': 'Remove words like \"very skilled\" ‚Üí \"skilled\"'\n",
        "        },\n",
        "        'personal_pronouns': {\n",
        "            'patterns': [\n",
        "                r'\\b(I|my|me|we|our|us)\\b',\n",
        "            ],\n",
        "            'message': 'Personal pronoun - remove for professional tone',\n",
        "            'examples': 'Remove \"I led\" ‚Üí \"Led\"'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    issues_found = []\n",
        "    text_lower = resume_text.lower()\n",
        "\n",
        "    for category, data in vague_patterns.items():\n",
        "        for pattern in data['patterns']:\n",
        "            matches = list(re.finditer(pattern, text_lower, re.IGNORECASE))\n",
        "            for match in matches:\n",
        "                start = max(0, match.start() - 50)\n",
        "                end = min(len(resume_text), match.end() + 50)\n",
        "                context = resume_text[start:end].strip()\n",
        "\n",
        "                # Identify which section this issue is in\n",
        "                section = identify_section_at_position(resume_text, match.start())\n",
        "\n",
        "                issues_found.append({\n",
        "                    'category': category.replace('_', ' ').title(),\n",
        "                    'phrase': match.group(),\n",
        "                    'context': context,\n",
        "                    'message': data['message'],\n",
        "                    'examples': data['examples'],\n",
        "                    'position': match.start(),\n",
        "                    'section': section  # NEW: Section identifier\n",
        "                })\n",
        "\n",
        "    issues_found.sort(key=lambda x: x['position'])\n",
        "    return issues_found\n",
        "\n",
        "# ============================================\n",
        "# 4. NEW FEATURE: ATS COMPLIANCE SCORING\n",
        "# ============================================\n",
        "\n",
        "def check_ats_compliance(resume_text):\n",
        "    \"\"\"Check if resume is ATS-friendly\"\"\"\n",
        "    compliance_score = 100\n",
        "    issues = []\n",
        "    warnings = []\n",
        "\n",
        "    # Check 1: Special characters and symbols\n",
        "    special_chars = re.findall(r'[‚òÖ‚òÜ‚óè‚óã‚ñ†‚ñ°‚ñ™‚ñ´‚óÜ‚óá]', resume_text)\n",
        "    if special_chars:\n",
        "        issues.append(f\"‚ùå Special characters found ({len(special_chars)}) - replace with standard text\")\n",
        "        compliance_score -= 15\n",
        "\n",
        "    # Check 2: Tables detection\n",
        "    if resume_text.count('|') > 10:\n",
        "        warnings.append(\"‚ö†Ô∏è Possible table formatting - ATS may misread columns\")\n",
        "        compliance_score -= 10\n",
        "\n",
        "    # Check 3: Contact information\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    phone_pattern = r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\n",
        "\n",
        "    has_email = bool(re.search(email_pattern, resume_text))\n",
        "    has_phone = bool(re.search(phone_pattern, resume_text))\n",
        "\n",
        "    if not has_email:\n",
        "        issues.append(\"‚ùå Email address not detected\")\n",
        "        compliance_score -= 15\n",
        "    if not has_phone:\n",
        "        warnings.append(\"‚ö†Ô∏è Phone number not clearly detected\")\n",
        "        compliance_score -= 5\n",
        "\n",
        "    # Check 4: Section headers\n",
        "    standard_sections = ['experience', 'education', 'skills', 'work', 'professional']\n",
        "    sections_found = sum(1 for section in standard_sections if section in resume_text.lower())\n",
        "\n",
        "    if sections_found < 2:\n",
        "        issues.append(\"‚ùå Missing standard section headers (Experience, Education, Skills)\")\n",
        "        compliance_score -= 20\n",
        "\n",
        "    # Check 5: Date formats\n",
        "    date_patterns = [\n",
        "        r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}\\b',\n",
        "        r'\\b\\d{1,2}/\\d{4}\\b',\n",
        "        r'\\b\\d{4}\\s*-\\s*\\d{4}\\b'\n",
        "    ]\n",
        "    dates_found = sum(1 for pattern in date_patterns if re.search(pattern, resume_text))\n",
        "\n",
        "    if dates_found == 0:\n",
        "        warnings.append(\"‚ö†Ô∏è No clear date formats found - add dates to experience\")\n",
        "        compliance_score -= 10\n",
        "\n",
        "    # Check 6: Acronyms\n",
        "    acronyms = re.findall(r'\\b[A-Z]{2,}\\b', resume_text)\n",
        "    if len(acronyms) > 10:\n",
        "        warnings.append(f\"‚ö†Ô∏è Many acronyms found ({len(acronyms)}) - consider expanding first use\")\n",
        "        compliance_score -= 5\n",
        "\n",
        "    # Check 7: Length\n",
        "    word_count = len(resume_text.split())\n",
        "    if word_count < 200:\n",
        "        issues.append(\"‚ùå Resume too short (< 200 words)\")\n",
        "        compliance_score -= 15\n",
        "    elif word_count > 1000:\n",
        "        warnings.append(\"‚ö†Ô∏è Resume lengthy (> 1000 words) - consider condensing\")\n",
        "        compliance_score -= 5\n",
        "\n",
        "    # Check 8: Hyperlinks\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    urls_found = len(re.findall(url_pattern, resume_text))\n",
        "    if urls_found > 3:\n",
        "        warnings.append(\"‚ö†Ô∏è Multiple URLs found - ATS may not process hyperlinks\")\n",
        "        compliance_score -= 5\n",
        "\n",
        "    return {\n",
        "        'score': max(0, compliance_score),\n",
        "        'issues': issues,\n",
        "        'warnings': warnings,\n",
        "        'word_count': word_count,\n",
        "        'has_email': has_email,\n",
        "        'has_phone': has_phone,\n",
        "        'sections_found': sections_found\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# 5. NEW FEATURE: RESUME REWRITE SUGGESTIONS\n",
        "# ============================================\n",
        "\n",
        "def generate_rewrite_suggestions(resume_text, jd_text, skill_gaps):\n",
        "    \"\"\"Generate specific rewrite suggestions based on job description\"\"\"\n",
        "    suggestions = []\n",
        "    jd_lower = jd_text.lower()\n",
        "\n",
        "    # Suggestion 1: Add missing skills\n",
        "    if skill_gaps['missing']:\n",
        "        top_missing = skill_gaps['missing'][:5]\n",
        "        suggestions.append({\n",
        "            'priority': 'HIGH',\n",
        "            'section': 'Skills',\n",
        "            'issue': f\"Missing {len(skill_gaps['missing'])} key skills from job description\",\n",
        "            'suggestion': f\"Add these skills if you have them: {', '.join(top_missing)}\",\n",
        "            'example': f\"Technical Skills: Python, SQL, {', '.join(top_missing[:3])}, ...\"\n",
        "        })\n",
        "\n",
        "    # Suggestion 2: Quantify achievements\n",
        "    vague_achievements = re.findall(\n",
        "        r'(improved|increased|reduced|enhanced|optimized|developed|created|led|managed)',\n",
        "        resume_text.lower()\n",
        "    )\n",
        "    if len(vague_achievements) > 3:\n",
        "        suggestions.append({\n",
        "            'priority': 'HIGH',\n",
        "            'section': 'Experience',\n",
        "            'issue': 'Multiple achievements lack quantifiable metrics',\n",
        "            'suggestion': 'Add specific numbers, percentages, or timeframes to show impact',\n",
        "            'example': 'Before: \"Improved system performance\"\\nAfter: \"Improved system performance by 45%, reducing load time from 3s to 1.5s\"'\n",
        "        })\n",
        "\n",
        "    # Suggestion 3: Match job title keywords\n",
        "    jd_titles = re.findall(r'(engineer|developer|analyst|manager|scientist|specialist)', jd_lower)\n",
        "    resume_lower = resume_text.lower()\n",
        "\n",
        "    if jd_titles and not any(title in resume_lower for title in jd_titles):\n",
        "        suggestions.append({\n",
        "            'priority': 'MEDIUM',\n",
        "            'section': 'Professional Summary',\n",
        "            'issue': 'Resume doesn\\'t emphasize role titles from job description',\n",
        "            'suggestion': f'Include relevant titles like: {\", \".join(set(jd_titles[:3]))}',\n",
        "            'example': f'Add: \"Experienced {jd_titles[0].title()} with expertise in...\"'\n",
        "        })\n",
        "\n",
        "    # Suggestion 4: Add action verbs\n",
        "    weak_verbs = len(re.findall(r'\\b(responsible for|worked on|helped)\\b', resume_text.lower()))\n",
        "    if weak_verbs > 2:\n",
        "        suggestions.append({\n",
        "            'priority': 'MEDIUM',\n",
        "            'section': 'Experience',\n",
        "            'issue': f'Found {weak_verbs} weak action verbs',\n",
        "            'suggestion': 'Replace with strong action verbs that show leadership and impact',\n",
        "            'example': 'Replace \"Responsible for managing\" with \"Led\" or \"Directed\"\\nReplace \"Worked on projects\" with \"Developed\" or \"Architected\"'\n",
        "        })\n",
        "\n",
        "    # Suggestion 5: Education requirements\n",
        "    if 'bachelor' in jd_lower or 'degree' in jd_lower:\n",
        "        if not re.search(r'(bachelor|master|phd|degree)', resume_text.lower()):\n",
        "            suggestions.append({\n",
        "                'priority': 'HIGH',\n",
        "                'section': 'Education',\n",
        "                'issue': 'Education section missing or not prominent',\n",
        "                'suggestion': 'Clearly list your degree(s) as job requires specific education',\n",
        "                'example': 'Bachelor of Science in Computer Science, XYZ University, 2020'\n",
        "            })\n",
        "\n",
        "    # Suggestion 6: Add projects\n",
        "    if 'project' in jd_lower and 'project' not in resume_text.lower():\n",
        "        suggestions.append({\n",
        "            'priority': 'MEDIUM',\n",
        "            'section': 'Projects',\n",
        "            'issue': 'No projects section found but job mentions project experience',\n",
        "            'suggestion': 'Add 2-3 relevant projects that demonstrate required skills',\n",
        "            'example': 'Projects:\\n‚Ä¢ NLP Chatbot - Built using Python, TensorFlow, achieving 90% accuracy\\n‚Ä¢ Data Pipeline - Processed 1M+ records using SQL and Python'\n",
        "        })\n",
        "\n",
        "    # Suggestion 7: Years of experience\n",
        "    years_required = re.findall(r'(\\d+)\\+?\\s*years?', jd_lower)\n",
        "    if years_required:\n",
        "        suggestions.append({\n",
        "            'priority': 'LOW',\n",
        "            'section': 'Professional Summary',\n",
        "            'issue': f'Job requires {years_required[0]} years experience',\n",
        "            'suggestion': 'Highlight your years of experience early in resume',\n",
        "            'example': f'Professional Summary: Software Engineer with {years_required[0]}+ years...'\n",
        "        })\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "# ============================================\n",
        "# 6. SECTION EXTRACTION\n",
        "# ============================================\n",
        "\n",
        "def extract_resume_sections(resume_text):\n",
        "    \"\"\"Extract different sections from resume\"\"\"\n",
        "    sections = {\n",
        "        'experience': '',\n",
        "        'education': '',\n",
        "        'skills': '',\n",
        "        'projects': '',\n",
        "        'other': resume_text\n",
        "    }\n",
        "\n",
        "    patterns = {\n",
        "        'experience': r'(work experience|professional experience|experience|employment history|work history)',\n",
        "        'education': r'(education|academic background|qualifications)',\n",
        "        'skills': r'(technical skills|skills|core competencies|expertise|technologies)',\n",
        "        'projects': r'(projects|key projects|notable projects|portfolio)'\n",
        "    }\n",
        "\n",
        "    text_lower = resume_text.lower()\n",
        "    section_positions = []\n",
        "\n",
        "    for section_name, pattern in patterns.items():\n",
        "        matches = list(re.finditer(pattern, text_lower))\n",
        "        for match in matches:\n",
        "            section_positions.append((match.start(), section_name))\n",
        "\n",
        "    section_positions.sort()\n",
        "\n",
        "    for i, (start_pos, section_name) in enumerate(section_positions):\n",
        "        if i < len(section_positions) - 1:\n",
        "            end_pos = section_positions[i + 1][0]\n",
        "            sections[section_name] = resume_text[start_pos:end_pos]\n",
        "        else:\n",
        "            sections[section_name] = resume_text[start_pos:]\n",
        "\n",
        "    return sections\n",
        "\n",
        "def analyze_section_strength(section_text, jd_text):\n",
        "    \"\"\"Analyze how well a resume section matches the job description\"\"\"\n",
        "    if not section_text or len(section_text.strip()) < 20:\n",
        "        return 0.0\n",
        "\n",
        "    section_embedding = model.encode([section_text])\n",
        "    jd_embedding = model.encode([jd_text])\n",
        "    similarity = cosine_similarity(section_embedding, jd_embedding)[0][0]\n",
        "\n",
        "    return round(similarity * 100, 2)\n",
        "\n",
        "# ============================================\n",
        "# 7. KEYWORD EXTRACTION\n",
        "# ============================================\n",
        "\n",
        "def extract_keywords(text, top_n=20):\n",
        "    \"\"\"Extract important keywords from text\"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    keywords = [\n",
        "        word for word in tokens\n",
        "        if word.isalnum() and word not in stop_words and len(word) > 2\n",
        "    ]\n",
        "\n",
        "    keyword_freq = Counter(keywords)\n",
        "    return keyword_freq.most_common(top_n)\n",
        "\n",
        "def find_matching_keywords(resume_text, jd_text):\n",
        "    \"\"\"Find common keywords between resume and job description\"\"\"\n",
        "    resume_keywords = set([kw[0] for kw in extract_keywords(resume_text, top_n=50)])\n",
        "    jd_keywords = set([kw[0] for kw in extract_keywords(jd_text, top_n=50)])\n",
        "    matching = resume_keywords.intersection(jd_keywords)\n",
        "    return sorted(list(matching))\n",
        "\n",
        "# ============================================\n",
        "# 8. SKILL GAP ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "def extract_technical_skills(text):\n",
        "    \"\"\"Extract technical skills from text\"\"\"\n",
        "    skill_patterns = [\n",
        "        r'\\b(python|java|javascript|typescript|c\\+\\+|c#|ruby|go|rust|swift|kotlin|php|r|matlab|scala)\\b',\n",
        "        r'\\b(react|angular|vue|node\\.?js|express|django|flask|spring|tensorflow|pytorch|keras|scikit-learn|pandas|numpy)\\b',\n",
        "        r'\\b(sql|mysql|postgresql|mongodb|redis|elasticsearch|oracle|sqlite|dynamodb)\\b',\n",
        "        r'\\b(aws|azure|gcp|docker|kubernetes|jenkins|terraform|ansible|git|github|gitlab)\\b',\n",
        "        r'\\b(machine learning|deep learning|nlp|computer vision|data science|neural networks|transformers|bert|gpt|llm)\\b',\n",
        "        r'\\b(jupyter|colab|tableau|power bi|excel|jira|confluence|slack)\\b',\n",
        "        r'\\b(html|css|rest api|graphql|websocket|ajax)\\b',\n",
        "        r'\\b(agile|scrum|ci/cd|microservices|api|backend|frontend|full stack)\\b'\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    skills = set()\n",
        "\n",
        "    for pattern in skill_patterns:\n",
        "        matches = re.findall(pattern, text_lower, re.IGNORECASE)\n",
        "        skills.update(matches)\n",
        "\n",
        "    return skills\n",
        "\n",
        "def analyze_skill_gaps(resume_text, jd_text):\n",
        "    \"\"\"Identify missing skills from job description\"\"\"\n",
        "    resume_skills = extract_technical_skills(resume_text)\n",
        "    jd_skills = extract_technical_skills(jd_text)\n",
        "\n",
        "    matching_skills = resume_skills.intersection(jd_skills)\n",
        "    missing_skills = jd_skills - resume_skills\n",
        "    extra_skills = resume_skills - jd_skills\n",
        "\n",
        "    return {\n",
        "        'matching': sorted(list(matching_skills)),\n",
        "        'missing': sorted(list(missing_skills)),\n",
        "        'extra': sorted(list(extra_skills)),\n",
        "        'match_percentage': round(len(matching_skills) / len(jd_skills) * 100, 2) if jd_skills else 0\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# 9. COMPATIBILITY SCORING\n",
        "# ============================================\n",
        "\n",
        "def calculate_compatibility(resume_text, jd_text):\n",
        "    \"\"\"Calculate semantic similarity between resume and JD\"\"\"\n",
        "    resume_embedding = model.encode([resume_text])\n",
        "    jd_embedding = model.encode([jd_text])\n",
        "    similarity = cosine_similarity(resume_embedding, jd_embedding)[0][0]\n",
        "    score = round(similarity * 100, 2)\n",
        "    return score\n",
        "\n",
        "# ============================================\n",
        "# 10. MAIN ANALYSIS FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def analyze_resume(file, job_description):\n",
        "    \"\"\"Main function to analyze resume compatibility\"\"\"\n",
        "\n",
        "    if file is None:\n",
        "        return \"‚ùå Please upload a resume (PDF or Word)\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    if not job_description or len(job_description.strip()) < 50:\n",
        "        return \"‚ùå Please enter a valid job description\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        resume_text = extract_text_from_file(file)\n",
        "\n",
        "        if resume_text.startswith(\"Error\") or resume_text.startswith(\"Unsupported\"):\n",
        "            return f\"‚ùå {resume_text}\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        if not resume_text or len(resume_text) < 50:\n",
        "            return \"‚ùå Could not extract sufficient text from file\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        clean_resume = clean_text(resume_text)\n",
        "        clean_jd = clean_text(job_description)\n",
        "\n",
        "        # Original analyses\n",
        "        overall_score = calculate_compatibility(clean_resume, clean_jd)\n",
        "        sections = extract_resume_sections(resume_text)\n",
        "        section_scores = {}\n",
        "        for section_name, section_text in sections.items():\n",
        "            if section_text and len(section_text.strip()) > 20:\n",
        "                score = analyze_section_strength(section_text, clean_jd)\n",
        "                section_scores[section_name] = score\n",
        "\n",
        "        matching_keywords = find_matching_keywords(clean_resume, clean_jd)\n",
        "        skill_gaps = analyze_skill_gaps(resume_text, job_description)\n",
        "\n",
        "        # New features\n",
        "        vague_language_issues = detect_vague_language(resume_text)\n",
        "        ats_compliance = check_ats_compliance(resume_text)\n",
        "        rewrite_suggestions = generate_rewrite_suggestions(resume_text, job_description, skill_gaps)\n",
        "\n",
        "        # Format results\n",
        "        score_text = f\"## üéØ Overall Compatibility Score: **{overall_score}%**\\n\\n\"\n",
        "        if overall_score >= 80:\n",
        "            score_text += \"‚úÖ **Excellent Match!**\"\n",
        "        elif overall_score >= 60:\n",
        "            score_text += \"‚úîÔ∏è **Good Match!**\"\n",
        "        elif overall_score >= 40:\n",
        "            score_text += \"‚ö†Ô∏è **Moderate Match.**\"\n",
        "        else:\n",
        "            score_text += \"‚ùå **Low Match.**\"\n",
        "\n",
        "        # Section Analysis\n",
        "        section_analysis = \"## üìä Section Analysis\\n\\n\"\n",
        "        if section_scores:\n",
        "            sorted_sections = sorted(section_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            section_analysis += \"| Section | Score | Status |\\n|---------|-------|--------|\\n\"\n",
        "            for section_name, score in sorted_sections:\n",
        "                emoji = \"üü¢\" if score >= 60 else \"üü°\" if score >= 40 else \"üî¥\"\n",
        "                status = \"Strong\" if score >= 60 else \"Moderate\" if score >= 40 else \"Weak\"\n",
        "                section_analysis += f\"| {section_name.title()} | {score}% | {emoji} {status} |\\n\"\n",
        "\n",
        "        # Skill Gaps\n",
        "        skill_gap_text = f\"## üéØ Skill Gap Analysis\\n\\n**Match Rate:** {skill_gaps['match_percentage']}%\\n\\n\"\n",
        "        if skill_gaps['matching']:\n",
        "            skill_gap_text += f\"### ‚úÖ Matching ({len(skill_gaps['matching'])})\\n\"\n",
        "            skill_gap_text += \", \".join([f\"**{s}**\" for s in skill_gaps['matching'][:15]]) + \"\\n\\n\"\n",
        "        if skill_gaps['missing']:\n",
        "            skill_gap_text += f\"### ‚ùå Missing ({len(skill_gaps['missing'])})\\n\"\n",
        "            skill_gap_text += \", \".join([f\"**{s}**\" for s in skill_gaps['missing'][:20]]) + \"\\n\\n\"\n",
        "\n",
        "        # Keywords\n",
        "        keywords_text = \"### üîë Matching Keywords:\\n\\n\"\n",
        "        if matching_keywords:\n",
        "            keywords_text += \", \".join([f\"**{kw}**\" for kw in matching_keywords[:20]])\n",
        "        else:\n",
        "            keywords_text += \"‚ö†Ô∏è No significant matches found\"\n",
        "\n",
        "        # Vague Language\n",
        "        vague_language_text = \"## üîç Vague Language Detector\\n\\n\"\n",
        "        if vague_language_issues:\n",
        "            vague_language_text += f\"**Found {len(vague_language_issues)} issues**\\n\\n\"\n",
        "\n",
        "            # Group by category AND section\n",
        "            issues_by_cat = {}\n",
        "            for issue in vague_language_issues[:20]:  # Show up to 20 issues\n",
        "                cat = issue['category']\n",
        "                if cat not in issues_by_cat:\n",
        "                    issues_by_cat[cat] = []\n",
        "                issues_by_cat[cat].append(issue)\n",
        "\n",
        "            for category, issues in issues_by_cat.items():\n",
        "                vague_language_text += f\"### ‚ö†Ô∏è {category} ({len(issues)} found)\\n\"\n",
        "                vague_language_text += f\"**Issue:** {issues[0]['message']}\\n\"\n",
        "                vague_language_text += f\"**Fix:** {issues[0]['examples']}\\n\\n\"\n",
        "\n",
        "                # Show examples with section information\n",
        "                vague_language_text += \"**Examples in your resume:**\\n\"\n",
        "                for issue in issues[:5]:  # Show up to 5 examples per category\n",
        "                    vague_language_text += f\"- **[{issue['section']}]** *\\\"{issue['phrase']}\\\"* in: ...{issue['context']}...\\n\"\n",
        "                vague_language_text += \"\\n\"\n",
        "        else:\n",
        "            vague_language_text += \"‚úÖ No major issues detected!\\n\"\n",
        "\n",
        "        # ATS Compliance\n",
        "        ats_text = f\"## ü§ñ ATS Compliance: **{ats_compliance['score']}%**\\n\\n\"\n",
        "        if ats_compliance['score'] >= 80:\n",
        "            ats_text += \"‚úÖ **Excellent** - ATS-friendly\\n\\n\"\n",
        "        elif ats_compliance['score'] >= 60:\n",
        "            ats_text += \"‚úîÔ∏è **Good** - Minor fixes needed\\n\\n\"\n",
        "        else:\n",
        "            ats_text += \"‚ö†Ô∏è **Needs Improvement**\\n\\n\"\n",
        "\n",
        "        if ats_compliance['issues']:\n",
        "            ats_text += \"### ‚ùå Issues:\\n\"\n",
        "            for issue in ats_compliance['issues']:\n",
        "                ats_text += f\"- {issue}\\n\"\n",
        "        if ats_compliance['warnings']:\n",
        "            ats_text += \"\\n### ‚ö†Ô∏è Warnings:\\n\"\n",
        "            for warning in ats_compliance['warnings']:\n",
        "                ats_text += f\"- {warning}\\n\"\n",
        "\n",
        "        # Rewrite Suggestions\n",
        "        rewrite_text = \"## ‚úçÔ∏è Rewrite Suggestions\\n\\n\"\n",
        "        if rewrite_suggestions:\n",
        "            high = [s for s in rewrite_suggestions if s['priority'] == 'HIGH']\n",
        "            medium = [s for s in rewrite_suggestions if s['priority'] == 'MEDIUM']\n",
        "\n",
        "            if high:\n",
        "                rewrite_text += \"### üî¥ HIGH PRIORITY\\n\\n\"\n",
        "                for i, sug in enumerate(high, 1):\n",
        "                    rewrite_text += f\"**{i}. {sug['section']}:** {sug['suggestion']}\\n\"\n",
        "                    rewrite_text += f\"```\\n{sug['example']}\\n```\\n\\n\"\n",
        "\n",
        "            if medium:\n",
        "                rewrite_text += \"### üü° MEDIUM PRIORITY\\n\\n\"\n",
        "                for i, sug in enumerate(medium, 1):\n",
        "                    rewrite_text += f\"**{i}. {sug['section']}:** {sug['suggestion']}\\n\\n\"\n",
        "        else:\n",
        "            rewrite_text += \"‚úÖ Looks good!\\n\"\n",
        "\n",
        "        # Preview\n",
        "        preview_text = \"### üìÑ Preview:\\n\\n\" + clean_resume[:2000]\n",
        "        if len(clean_resume) > 2000:\n",
        "            preview_text += \"...\"\n",
        "\n",
        "        return (score_text, section_analysis, skill_gap_text, keywords_text,\n",
        "                vague_language_text, ats_text, rewrite_text, preview_text, clean_resume)\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"‚ùå Error: {str(e)}\\n\\n{traceback.format_exc()}\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "# ============================================\n",
        "# 11. GRADIO INTERFACE\n",
        "# ============================================\n",
        "\n",
        "SAMPLE_JD = \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'Arial', sans-serif;\n",
        "}\n",
        ".output-markdown h2 {\n",
        "    color: #2c3e50;\n",
        "    border-bottom: 2px solid #3498db;\n",
        "    padding-bottom: 10px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css, title=\"Enhanced Resume Analyzer\") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "# Enhanced Resume Analyzer\n",
        "\n",
        "Evaluates resume‚Äìjob fit beyond keywords, analyzes language quality, estimates ATS readiness, and suggests targeted improvements.\n",
        "\n",
        "### üß≠ How to Use\n",
        "1. Upload your resume (PDF or Word)\n",
        "2. Paste the full job description\n",
        "3. Click **Analyze**\n",
        "4. Review results tab-by-tab, starting with **Overview**\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "\n",
        "\n",
        "            file_input = gr.File(\n",
        "                label=\"Upload Resume (PDF/Word)\",\n",
        "                file_types=[\".pdf\", \".docx\"],\n",
        "                type=\"filepath\"\n",
        "            )\n",
        "\n",
        "            jd_input = gr.Textbox(\n",
        "                label=\"Job Description\",\n",
        "                placeholder=\"Paste job description here...\",\n",
        "                lines=10,\n",
        "                value=SAMPLE_JD\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                analyze_btn = gr.Button(\"üîç Analyze\", variant=\"primary\", size=\"lg\")\n",
        "                clear_btn = gr.Button(\"üîÑ Clear\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### üìä Results\")\n",
        "\n",
        "            with gr.Tabs():\n",
        "                with gr.Tab(\"üìà Overview\"):\n",
        "                    score_output = gr.Markdown()\n",
        "                    section_output = gr.Markdown()\n",
        "\n",
        "                with gr.Tab(\"üéØ Skills\"):\n",
        "                    skill_gap_output = gr.Markdown()\n",
        "                    keywords_output = gr.Markdown()\n",
        "\n",
        "                with gr.Tab(\"üîç Language\"):\n",
        "                    vague_language_output = gr.Markdown()\n",
        "\n",
        "                with gr.Tab(\"ü§ñ ATS\"):\n",
        "                    ats_output = gr.Markdown()\n",
        "\n",
        "                with gr.Tab(\"‚úçÔ∏è Suggestions\"):\n",
        "                    rewrite_output = gr.Markdown()\n",
        "\n",
        "                with gr.Tab(\"üìÑ Preview\"):\n",
        "                    preview_output = gr.Markdown()\n",
        "\n",
        "            full_text_output = gr.Textbox(visible=False)\n",
        "\n",
        "    # Connect buttons\n",
        "    analyze_btn.click(\n",
        "        fn=analyze_resume,\n",
        "        inputs=[file_input, jd_input],\n",
        "        outputs=[score_output, section_output, skill_gap_output, keywords_output,\n",
        "                vague_language_output, ats_output, rewrite_output, preview_output, full_text_output]\n",
        "    )\n",
        "\n",
        "    def clear_all():\n",
        "      return None, SAMPLE_JD, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "# This should be at the same indentation level as the function above\n",
        "    clear_btn.click(\n",
        "        fn=clear_all,\n",
        "        inputs=[],\n",
        "        outputs=[file_input, jd_input, score_output, section_output, skill_gap_output,\n",
        "                keywords_output, vague_language_output, ats_output, rewrite_output,\n",
        "                preview_output, full_text_output]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "---\n",
        "**Built as an applied NLP system for resume‚Äìjob alignment analysis.**\n",
        "Heuristic-based; not a hiring, ranking, or evaluation tool.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Launch\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üöÄ Launching Enhanced Resume Analyzer...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}